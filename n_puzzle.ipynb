{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Computaional Intelligence 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "from queue import *\n",
    "from random import *\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE_DIM = 2\n",
    "RANDOMIZE_STEPS = 100_000\n",
    "\n",
    "action = namedtuple('Action', ('pos1', 'pos2'))\n",
    "\n",
    "# Define a Node for gougeous visual plot of solution path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   Default game: search for number ordered increasing\n",
    "default_game = True \n",
    "\n",
    "#   If we wanna search a Specific configuration, ENTER here the configuration following the rules:\n",
    "#   -   MUST contain ONLY one \"0\";\n",
    "#   -   write number from 1 to PUZZLE_DIM**2 - 1 in a list\n",
    "\n",
    "GOAL_STATE = [1, 2, 3, 0]\n",
    "\n",
    "if not default_game:\n",
    "    if GOAL_STATE.count(0) != 1:\n",
    "        print(\"You insered too many 0s in your goal state!\\n\", GOAL_STATE)\n",
    "        exit(1)\n",
    "\n",
    "    if len(set(GOAL_STATE)) != PUZZLE_DIM**2:\n",
    "        print(\"You haven't created a feasible solution!\\n\", GOAL_STATE)\n",
    "        exit(1)\n",
    "\n",
    "    GOAL_STATE = np.array(GOAL_STATE).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "else:\n",
    "    GOAL_STATE = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "\n",
    "GOAL_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> list['Action']:\n",
    "    x, y = [int(_[0]) for _ in np.where(state == 0)]\n",
    "    actions = list()\n",
    "    if x > 0:\n",
    "        actions.append(action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "def do_action(state: np.ndarray, action: 'Action') -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "#   Initial state of the game   #\n",
    "INITIAL_STATE = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "for _ in range(RANDOMIZE_STEPS):\n",
    "    INITIAL_STATE = do_action(INITIAL_STATE, choice(available_actions(INITIAL_STATE)))\n",
    "print(INITIAL_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_check(state: np.ndarray) -> bool:\n",
    "    return np.array_equal(state, GOAL_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avoid_loop(state: np.ndarray, visited_state: list[np.ndarray]) -> bool:\n",
    "    return any(np.array_equal(state, s) for s in visited_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Randomizing:   0%|          | 11/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 11 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "state = INITIAL_STATE\n",
    "\n",
    "for r in tqdm(range(RANDOMIZE_STEPS), desc='Randomizing'):\n",
    "    #print(state)\n",
    "    state = do_action(state, choice(available_actions(state)))\n",
    "    if solution_check(state):\n",
    "        break\n",
    "print(f\"Solved in {r:,} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATH SEARCH STRATEGY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mi serve tener conto di:\n",
    "- Costo --> Quanti nodi ho esplorato -> nodi = azioni fatte per cambiare lo stato del gioco\n",
    "- Qualità --> Minor numero di nodi necessari per arrivare alla soluzione -> nodi = azioni\n",
    "\n",
    "OBIETTIVO --> ___Qualità vs Costo___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth-First Strategy\n",
    "\n",
    "Esploro in profondità ciascun nodo fino alla foglia, poi passo al figlio successivo.<br>\n",
    "Criticità -> Soluzioni :\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Depth-First: 2it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [3 2]] \n",
      "\n",
      "2\n",
      "[[1 0]\n",
      " [3 2]] \n",
      "\n",
      "2\n",
      "Solved in 3 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "frontier = deque([INITIAL_STATE])\n",
    "\n",
    "explored_state = list()\n",
    "#   action_sequence = list()\n",
    "state = frontier.pop()\n",
    "explored_state.append(state)\n",
    "explored_node = 1\n",
    "\n",
    "with tqdm(total=None, desc=\"Depth-First\") as pbar:\n",
    "    while not solution_check(state):\n",
    "        print(state, \"\\n\")\n",
    "        for a in available_actions(state):\n",
    "            new_state = do_action(state, a)\n",
    "            if not avoid_loop(new_state, explored_state):\n",
    "                frontier.appendleft(new_state)\n",
    "\n",
    "        print(len(frontier))\n",
    "        state = frontier.popleft()\n",
    "        explored_state.append(state)\n",
    "        explored_node += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "print(f\"Solved in {explored_node:,} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breadth-First Strategy\n",
    "\n",
    "Esploro prima tutti i figli di nodo allo stesso livello, poi passo ai figli, fino alle foglie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Depth-First: 7it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 8 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "frontier = deque([INITIAL_STATE])\n",
    "\n",
    "explored_state = list()\n",
    "#   action_sequence = list()\n",
    "state = frontier.pop()\n",
    "explored_state.append(state)\n",
    "explored_node = 1\n",
    "\n",
    "with tqdm(total=None, desc=\"Depth-First\") as pbar:\n",
    "    \n",
    "    while not solution_check(state):\n",
    "        actions = available_actions(state)\n",
    "\n",
    "        for a in actions:\n",
    "            new_state = do_action(state, a)\n",
    "            if not avoid_loop(new_state, explored_state):\n",
    "                frontier.append(new_state)\n",
    "\n",
    "        #print(len(frontier))\n",
    "        state = frontier.popleft()\n",
    "        explored_state.append(state)\n",
    "        explored_node += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "print(f\"Solved in {explored_node:,} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A* Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic(state: np.ndarray) -> int:\n",
    "    '''\n",
    "        Use the classic distance definition, of each point from its correct position, as cost\n",
    "    '''\n",
    "    n = state.shape[0]\n",
    "    distance = 0\n",
    "\n",
    "    for (i, j), tile in np.ndenumerate(state):\n",
    "        if tile == 0:\n",
    "            continue\n",
    "\n",
    "        target_i, target_j = divmod(tile-1, n)\n",
    "        distance += min(abs(i - target_i) + abs(j - target_j))\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, board: np.ndarray, previous_state: \"Node\") -> None:\n",
    "        self.board = board      # Game state\n",
    "        self.previous_state = previous_state    # Parent Node\n",
    "        if previous_state is None:      # Actual Cost to reach this Node\n",
    "            self.g = 0\n",
    "        else:\n",
    "            self.g = previous_state.g\n",
    "        self.f = (self.g + 1) + (heuristic(board))    # f(x) = g(x) + h(x)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.g, self.f) == (other.g, other.f)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return (self.g, self.f) < (other.g, other.f)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Rappresentazione leggibile del nodo.\n",
    "        \"\"\"\n",
    "        stato_str = '\\n'.join([' '.join(map(str, riga)) for riga in self.stato])\n",
    "        return f\"Stato:\\n{stato_str}\\nEuristica: {self.euristica}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_path_search(frontier: list[(np.ndarray, int)], state: tuple[np.ndarray, int] ) -> tuple[np.ndarray, int] :\n",
    "    return None, 0\n",
    "\n",
    "def better_path(state: tuple[np.ndarray, int], explored_state: list[(np.ndarray, int)]) -> list[(np.ndarray, int)]:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m frontier \u001b[38;5;241m=\u001b[39m PriorityQueue()\n\u001b[0;32m      3\u001b[0m explored_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINITIAL_STATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m explored_state\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n",
      "Cell \u001b[1;32mIn[129], line 9\u001b[0m, in \u001b[0;36mNode.__init__\u001b[1;34m(self, board, previous_state)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m previous_state\u001b[38;5;241m.\u001b[39mg\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[43mheuristic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[128], line 13\u001b[0m, in \u001b[0;36mheuristic\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     target_i, target_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(tile\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n)\n\u001b[1;32m---> 13\u001b[0m     distance \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_i\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_j\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distance\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "frontier = PriorityQueue()\n",
    "\n",
    "explored_state = list()\n",
    "state = Node(INITIAL_STATE, None)\n",
    "explored_state.append(state)\n",
    "\n",
    "with tqdm(total=None) as pbar:\n",
    "\n",
    "    while not solution_check(state.board):\n",
    "        for a in available_actions(state[0]):\n",
    "            new_state = do_action(state.board, a)\n",
    "            frontier.put(Node(new_state, state))\n",
    "\n",
    "        state = frontier.get()\n",
    "        explored_state.append(state)\n",
    "        explored_node += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "print(f\"Solved in {explored_node:,} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 0]] \n",
      " [[1 2]\n",
      " [3 0]]\n"
     ]
    }
   ],
   "source": [
    "print(state,\"\\n\", GOAL_STATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
